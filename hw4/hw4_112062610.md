---
title: '2024 Parallel Programming HW4 [112062610 劉得崙]'

---

# 2024 Parallel Programming HW4 [112062610 劉得崙]

> Flash attention illustration
>![1_i-MeAwCRNds5prU9HiSmuQ](https://hackmd.io/_uploads/HJ0pHYKHyx.png)


## 1. Implementation

#### a. Describe how you implemented the FlashAttention forward pass.
- Sram size
Qi : Br * d * sizeof(float)
Ki : Bc * d * sizeof(float)
Vi : Bc * d * sizeof(float)
Sij: Br * Bc * sizeof(float)
>![image](https://hackmd.io/_uploads/SkotHKKrJl.png)
>![image](https://hackmd.io/_uploads/HJPSrKtB1e.png)

- ==S = Q * K^T==
>![S__10977289](https://hackmd.io/_uploads/Sylh8Ftr1g.jpg)
>![image](https://hackmd.io/_uploads/SkxtUtFBkl.png)

- Record ==warp max== (mi_tilde) and ==partial sum== (li_tilde)
>![image](https://hackmd.io/_uploads/r1GlvtKrkg.png)

- ==O = S * V==. Incrementally update d_O.
>![S__10977291](https://hackmd.io/_uploads/Sy3ztKYS1g.jpg)
>![image](https://hackmd.io/_uploads/BJ1HwtFS1x.png)


#### b. Explain how matrices Q, K, and V are divided into blocks and processed in parallel.
- Q is divided to ==Tr = ceil(N / Br)==
- K, V is diviced to ==Tc = ceil(N/Bc)==

After loading the shared memory, each thread can access Ki and Vi in parallel.
>![S__10977288](https://hackmd.io/_uploads/r1q1UKYrkx.jpg)

#### c. Describe how you chose the block sizes B_r and B_c and why.
- Br: As large as possible to fully utilize the hardware.
- Bc: Not to exceed the size of sram.

#### d. Specify the configurations for CUDA kernel launches, such as the number of threads per block, shared memory allocation, and grid dimensions.
- threads per block : 128
- grid dimensions : Tr * batch_size
>![image](https://hackmd.io/_uploads/Hkbm2tYr1e.png)

#### e. Justify your choices and how they relate to the blocking factors and the SRAM size.
- 128 * 16 -> pass
>![image](https://hackmd.io/_uploads/SyD2I5tB1l.png)
- 128 * 16 + PAD -> fail
>![image](https://hackmd.io/_uploads/rkS6Uqtryl.png)
- 128 * 15 + PAD -> pass
>![image](https://hackmd.io/_uploads/SyLUP9tHkl.png)


## 2. Profiling Results (hw3-2)
- Command
```
make hw4
srun -pnvidia -N1 -n1 --gres=gpu:1 \
nvprof --metrics achieved_occupancy,sm_efficiency,\
gld_throughput,gst_throughput,shared_load_throughput,shared_store_throughput \
./hw4 /share/testcases/hw4/t20 t20.out
```
- Output (flash_attention_kernel)

| Metric | Min | Max | Average |
|-|-|-|-|
| Achieved Occupancy | 0.046761 | 0.046761 | 0.046761 |
| Multiprocessor Activity | 99.68% | 99.68% | 99.68% |
| Global Load Throughput | 42.192GB/s | 42.192GB/s | 42.192GB/s |
| Global Store Throughput | 13.980GB/s | 13.980GB/s | 13.980GB/s |
| Shared Memory Load Throughput | 3718.6GB/s | 3718.6GB/s | 3718.6GB/s |
| Shared Memory Store Throughput | 168.63GB/s | 168.63GB/s | 168.63GB/s |

## 3. Experiment & Analysis
#### a. System Spec
> Apollo-gpu workstation

#### b. Optimization
> - Command
> ```
> hw4-judge --debug -i t20
> ```
>![image](https://hackmd.io/_uploads/rkyqvsFrJg.png)


## 4. Experience & Conclusion
Through this homework, I gained deep insights into modern GPU programming and attention mechanisms.

The implementation of FlashAttention revealed the critical importance of memory hierarchy optimization - particularly the interplay between HBM and SRAM that significantly impacts performance.Working with CUDA for matrix operations taught me practical skills in tiling, blocking, and kernel fusion techniques essential for efficient parallel computing.

The assignment demonstrated how theoretical concepts in AI algorithms translate to hardware-level optimizations, especially in handling large-scale matrix operations.The experience of tuning thread organizations, managing shared memory, and optimizing memory access patterns provided valuable lessons in balancing computational efficiency with hardware constraints. 

Perhaps most importantly, this homework bridged the gap between theoretical understanding and practical implementation of state-of-the-art attention mechanisms, showing how algorithmic innovations can address hardware limitations.
