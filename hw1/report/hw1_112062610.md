---
title: '2024 Parallel Programming HW1 [112062610 劉得崙]'

---

# 2024 Parallel Programming HW1 [112062610 劉得崙] 

:::info
### Problem Description

> In this assignment, you are required to implement the odd-even sort algorithm using
MPI. Odd-even sort is a comparison sort that consists of two main phases:
even-phase and odd-phase. In each phase, processes perform compare-and-swap
operations repeatedly as follows until the input array is sorted.
:::

# Implementation
### Diagram
![S__10649607](https://hackmd.io/_uploads/Syj2GOWeJx.jpg)

### 0. Handling arbitrary elements and processes
Distrubute elements evenly, and store the count in variable ==self_count==.
If the process count is larger than the element count, ==rank_endpoint== will be smaller than ==size==, which is used to prevent deadlock.

```
int rank_endpoint = min(size, N);                           // process with 0 count 
int remainder = N % size;
int self_count = N / size + (rank < remainder);             // distribute remainder
int offset = N / size * rank + min(rank, remainder);        // starting point in file
int left_count = self_count + (rank == remainder);          // left task's count
int right_count = self_count - (rank + 1 == remainder);     // right task's count
```

### 1. Read file
Use ==MPI_File_open== and ==MPI_File_read_at== to optimize reading process. Use ==offset== to distinguish each process's starting point.
```
MPI_File input_file;
MPI_File_open(MPI_COMM_WORLD, argv[2], MPI_MODE_RDONLY, MPI_INFO_NULL, &input_file);&output_file);
MPI_File_read_at(input_file, offset * sizeof(float), self_arr, self_count, MPI_FLOAT, MPI_STATUS_IGNORE);
MPI_File_close(&input_file);
```

### 2. Local sort
Use ==boost::sort::spreadsort::float_sort== for better performance. Use ==omp== for possible compiler optimization.

```
#include <boost/sort/spreadsort/float_sort.hpp>

#pragma omp parallel
{
    #pragma omp single
    boost::sort::spreadsort::float_sort(self_arr, self_arr + self_count);
}
```

### 3 & 4. Odd-even merging process
Use ==MPI_Allreduce== for termination. For better performance, check only once every 4 iterations.
```
int global_swapped = 1, local_swapped = 0, iteration = 1;
while (global_swapped)
{
    // even merge
    
    local_swapped = 0;
    
    // odd merge
    
    if (!(iteration & 3)) MPI_Allreduce(&local_swapped, &global_swapped, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);
    else global_swapped = 1;
    iteration += 1;
}
```

>### Even merge
>In even merge, ==even indices== are ==left part==, ==odd indices== are ==right part==.
>Use ==rank_endpoint== to exclude processes with 0 count.

```
if (!(rank & 1) && rank < rank_endpoint - 1) // left part
{   
    // left part's action
}
else if (rank & 1 && rank < rank_endpoint) // right
{
    // right part's action
}
```

>### Odd merge
>In odd merge, ==odd indices== are ==left part==, ==even indices== are ==right part==.
```
if ((rank & 1) && rank < rank_endpoint - 1) // left part
{
    // left part's action
}
else if (!(rank & 1) && rank != 0 && rank < rank_endpoint) // right part
{
    // right part's action
}
```

>### Left part
>Use ==MPI_Sendrecv== for passing data with partner. For better performance, pass just ==front-most of the right array== or ==rear-most of the left array== for early decision.
```
// loads one element first
MPI_Sendrecv(self_arr + self_count - 1, 1, MPI_FLOAT, rank + 1, 0, 
        partner_arr, 1, MPI_FLOAT, rank + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);

// loads the whole data
if (self_arr[self_count-1] > partner_arr[0]) // meaning unsorted
{
    MPI_Sendrecv(self_arr, self_count - 1, MPI_FLOAT, rank + 1, 0, 
            partner_arr + 1, right_count - 1, MPI_FLOAT, rank + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);

    local_swapped = front_merge(self_arr, partner_arr, buff_arr, self_count, right_count);
}
```

>### Right part
>Use ==MPI_Sendrecv== for passing data with partner.
```
// loads one element first
MPI_Sendrecv(self_arr, 1, MPI_FLOAT, rank - 1, 0,
            partner_arr + left_count - 1, 1, MPI_FLOAT, rank - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);

// loads the whole data
if (partner_arr[left_count-1] > self_arr[0])
{
    MPI_Sendrecv(self_arr + 1, self_count - 1, MPI_FLOAT, rank - 1, 0,
            partner_arr, left_count - 1, MPI_FLOAT, rank - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);

    rear_merge(partner_arr, self_arr, buff_arr, left_count, self_count);
}
```

>### Front merge
>Normal merge, stop once reaching the middle point. Use ==pointer arithmetic== for faster addressing.

```
int front_merge(float*& left, float* right, float*& buffer, int left_count, int right_count)
{
    int swapped = 0;
    float *i = left, *j = right, *k = buffer;
    float *const iend = i + left_count, *const jend = j + right_count, *const kend = k + left_count;
    while (k != kend)
    {   // cuz jend is smaller
        if (j == jend || *i <= *j) *k++ = *i++;
        else 
        {
            *k++ = *j++;
            swapped = 1;
        }
    }
    std::swap(left, buffer);
    return swapped;
}
```

>### Rear merge
>Merge from the back of the array. Stop once reaching the middle point.

```
int rear_merge(float* left, float*& right, float*& buffer, int left_count, int right_count)
{
    int swapped = 0;
    float *i = left + left_count - 1, *j = right + right_count - 1, *k = buffer + right_count - 1;
    float *const iend = left - 1, *const jend = right - 1, *const kend = buffer - 1;
    while (k != kend)
    {   // cuz jend is smaller
        if (*j >= *i) *k-- = *j--;
        else 
        {
            *k-- = *i--;
            swapped = 1;
        }
    }
    std::swap(right, buffer);
    return swapped;
}
```

### 5. Write file
Use ==MPI_File_open== and ==MPI_File_write_at== to optimize writing process. Use ==offset== to distinguish each process's starting point.
```
MPI_File output_file;
MPI_File_open(MPI_COMM_WORLD, argv[3], MPI_MODE_CREATE | MPI_MODE_WRONLY, MPI_INFO_NULL, &output_file);
MPI_File_write_at(output_file, offset * sizeof(float), self_arr, self_count, MPI_FLOAT, MPI_STATUS_IGNORE);
MPI_File_close(&output_file);
```

# Experiment & Analysis
### 1. Methodology
>#### a. System Spec

Apollo Cluster and Slurm manager provided by the class.

>#### b. Performance Metrics

Use ==Nsight System== provided by NVIDIA for CPU time, communication time and IO time measurement.

Command: (use testcase 33 for example)
```
srun -N1 -n12 \
nsys profile \
    -o "./nsys_reports/rank_$PMI_RANK.nsys-rep" \
    --mpi-impl openmpi \
    --trace mpi,ucx,osrt \ 
    ./hw1 536869888 /home/pp24/share/hw1/testcases/33.in 33.out
```
---
### 2. Plots: Speedup Factor & Profile
>#### a. Different Implementation

- **Test data size**: 536869888
- **Process size**: 12
- **Node count**: 1
- **Core count**: 1 (default)

|  | hw1_v3.cc (before) | hw1_v17.cc (after)  |
|-|-|-|
| initial sorting | std::sort | boost::sort::spreadsort::float_sort  |
| merging | vector indexing +  transfer all data no checking +  merge the whole array | pointer arithmetic +  transfer one data for checking +  merge half of the array  |
| data exchange | MPI_Send / MPI_Recv | MPI_Sendrecv  |
| check for termination | MPI_Allreduce in each iteration | MPI_Allreduce once every 4 iteration  |

![image](https://hackmd.io/_uploads/ry6o96MeJe.png)

---

>#### b. Different Processes Count / Nodes count

| \# of process | 1 | 4 | 16 | 64  |
|-|-|-|-|-|
| Nodes used | 1 | 1 | 2 | 6  |


![image](https://hackmd.io/_uploads/HyWEoTfxke.png)

- **Speedup** = ==old total time== / ==new total time==

![image](https://hackmd.io/_uploads/Syb8-CMlyl.png)

---

### 3. Discussion

- **Bottleneck**: ==I/O operation==. As the # of process goes up, I/O operation starts to occupy more than 2/3 of total execution time. **Potential solution**: ==Buffered I/O==, this reduces the number of system calls and improves efficiency, ==Increase buffer size==, this reduces the number of I/O operations.
- **Scalabitily**: CPU time decreases as the # of process goes up; however, communication time also goes up during the process. For 16 processes and 64 processes, the decrease in total time is not that significant anymore.

# Conclusion

This homework highlighted some crucial aspects of MPI implementation. For instance, there are two ways of dividing works in MPI, one is ==calculate start-end== method, and the other is ==stepping size== method, diagram shown below.
![S__10657795](https://hackmd.io/_uploads/Hk-e11mxkg.jpg)

Normally, ==stepping size== method will have slightly better performance since it doesn't require work distribution phase at the start. However in this case, if we stay true to the sequential code (meaning it's essentially ==odd-even swap== operation), the performance is worse due to the slower transmission speed of elements from end to end.
![S__10657796](https://hackmd.io/_uploads/HkOmxkme1e.jpg)

But with ==calculate start-end== method, there's a merge function that can sort two arrays in a more efficient way, resulting in a better performance. However, it is an ==odd-even merge== operation, different from the original sequential version.
![S__10657797](https://hackmd.io/_uploads/BJgpeyXlyg.jpg)

---