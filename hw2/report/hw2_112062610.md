---
title: '2024 Parallel Programming HW2 [112062610 劉得崙]'

---

# 2024 Parallel Programming HW2 [112062610 劉得崙] 

:::info
### Problem Description

> In this assignment, you are asked to parallelize the sequential Mandelbrot Set
program, and learn the following skills:
● Get familiar with thread programming using Pthread and OpenMP.
● Combine process and thread to implement a hybrid parallelism solution .
● Understand the importance of load balance.
![image](https://hackmd.io/_uploads/Bk1_D6gbkx.png)
:::

# Overview
### Problem size
- (width * height) pixels to calculate
![S__10706949](https://hackmd.io/_uploads/rJPy56eZyg.jpg)

### Vectorization (in each row)
- Intel AVX-512 and AVX instructions
![S__10706947](https://hackmd.io/_uploads/B1hYsTebyx.jpg)


# [Pthread] Implementation
### 0. Diagram
- Create a ==task queue== for each ==row==, do the dynamic load balancing with pthread.
![S__10715145](https://hackmd.io/_uploads/r1PY9OMZJl.jpg)


### 1. Task queue for rows
- Create a std::vector for rows. Use ==current_row== as iterator, iterate over the queue.
- Create a mutex with ==pthread_mutex_t==, and use ==pthread_mutex_lock== and ==pthread_mutex_unlock== for critical section. This achieves ==dynamic load balancing==.
```
class TaskQueue {
private:
    std::vector<int> rows;
    pthread_mutex_t mutex;
    size_t current_row; 

public:
    TaskQueue(int height) : current_row(0) {
        pthread_mutex_init(&mutex, nullptr);
        // Create tasks for each row
        rows.resize(height);
        int idx = 0;
        for (auto& x : rows) x = idx++;
    }

    ~TaskQueue() {
        pthread_mutex_destroy(&mutex);
    }

    // Get next row to process. Returns -1 if no more rows.
    int getNextRow() {
        pthread_mutex_lock(&mutex);
        int row = -1;
        if (current_row < rows.size()) {
            row = rows[current_row++];
        }
        pthread_mutex_unlock(&mutex);
        return row;
    }
};
```

### 2. MandelbrotGenerator class
- ```struct ThreadData```: For passing in arguments.
- ==static void* wrapper(void* arg)==: For pthread_create() callback function type. Use ==static_cast<ThreadData*>->obj->compute()== for using the worker function.
- ```void compute(int thread_id)```: The actual worker function.
```
class MandelbrotGenerator
{
private:
    double left, right, lower, upper;
    int width, height, iters, thread_num;
    std::unique_ptr<int[]> image;
    std::unique_ptr<TaskQueue> task_queue;

    struct ThreadData
    {
        MandelbrotGenerator* obj;
        int thread_id;
    };

    static void* wrapper(void* arg)
    {
        ThreadData* data = static_cast<ThreadData*>(arg);
        data->obj->compute(data->thread_id);
        return nullptr;
    }

    void compute(int thread_id)
    {
        /* Worker's task */
    }
    
    /* public members */
};
```
- Create ==task_queue== and ==image== in the constructor, which is done by master thread.
- ==generate()== is the entry point for master thread to create multithreads.
- Pass in =={this, i}== as argument so that the ==static== wrapper() function can access the ==member== compute() function.
```
class MandelbrotGenerator
{
    /* private members */
public:
    MandelbrotGenerator(double l, double r, double low, double up, int w, int h, int iters, int thread_num)
        : left(l), right(r), lower(low), upper(up), width(w), height(h), iters(iters), thread_num(thread_num)
    {
        image = std::make_unique<int[]>(width * height);
        task_queue = std::make_unique<TaskQueue>(height);
    }

    void generate()
    {
        pthread_t threads[thread_num]; // For thread instances
        ThreadData* thread_data = new ThreadData[thread_num];

        // Create threads and compute
        for (int i = 0; i < thread_num; ++i)
        {
            thread_data[i] = {this, i};
            pthread_create(&threads[i], nullptr, wrapper, &thread_data[i]); 
        }

        // Wait for all
        for (int i = 0; i < thread_num; ++i)
        {
            pthread_join(threads[i], nullptr);
        }
    }

    void saveToPNG(const std::string& filename) const
    {
        PNGWriter writer(filename, iters, width, height, image.get());
        writer.write();
    }
};
```
### 3. Worker's task
- The function type is ==void (MandelbrotGenerator::*)(int)==; thus needs a wrapper function for passing into pthread_create().
- Keep requesting for new task until there's no more task to process. Done by ==while(true)== and ==task_queue->getNestRow()==.
- For vectorization, uses ==AVX-512==(512 bit) and ==AVX==(256 bit) instructions.
- Finish the remaining work (cannot be divided by 8) with normal calculation.
- Since ==image== is a private member of this class, the computed data can be stored directly into it by ==_mm256_storeu_epi32(&image[j * width + i], vec_repeats)==.

```
void compute(int thread_id)
{
    double x_offset = (right - left) / width;
    double y_offset = (upper - lower) / height;

    const __m512d vec_two = _mm512_set1_pd(2.0);
    const __m512d vec_four = _mm512_set1_pd(4.0);
    const __m512d vec_x_offset = _mm512_set1_pd(x_offset);
    const __m512d vec_left = _mm512_set1_pd(left);

    while (true)
    {
        int j = task_queue->getNextRow();
        if (j == -1) break;

        double y0 = j * y_offset + lower;
        __m512d vec_y0 = _mm512_set1_pd(y0);

        int i;
        for (i = 0; i < width - 7; i += 8)
        {
            __m512d vec_x0 = _mm512_fmadd_pd(
                _mm512_set_pd(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i),
                vec_x_offset,
                vec_left
            );

            __m512d vec_x = _mm512_setzero_pd();
            __m512d vec_y = _mm512_setzero_pd();
            __m512d vec_x2 = _mm512_setzero_pd();
            __m512d vec_y2 = _mm512_setzero_pd();
            __m512d vec_length_squared = _mm512_setzero_pd();
            __m256i vec_repeats = _mm256_setzero_si256();
            __mmask8 mask = 0xFF;

            for (int iter = 0; iter < iters; ++iter)
            {
                vec_x2 = _mm512_mul_pd(vec_x, vec_x);
                vec_y2 = _mm512_mul_pd(vec_y, vec_y);
                vec_length_squared = _mm512_add_pd(vec_x2, vec_y2);
                mask = _mm512_cmp_pd_mask(vec_length_squared, vec_four, _CMP_LT_OS);
                if (!mask) break;

                __m512d vec_2xy = _mm512_mul_pd(_mm512_mul_pd(vec_x, vec_y), vec_two);
                vec_x = _mm512_add_pd(_mm512_sub_pd(vec_x2, vec_y2), vec_x0);
                vec_y = _mm512_add_pd(vec_2xy, vec_y0);

                vec_repeats = _mm256_mask_add_epi32(
                    vec_repeats,   
                    mask,            
                    vec_repeats,     
                    _mm256_set1_epi32(1) 
                );
            }
            _mm256_storeu_epi32(&image[j * width + i], vec_repeats);
        }
        
        // Finish the remaining
        for (; i < width; ++i)
        {
            double x0 = i * x_offset + left;
            double x = 0;
            double y = 0;
            double length_squared = 0;
            int repeats = 0;
            while (repeats < iters && length_squared < 4)
            {
                double temp = x * x - y * y + x0;
                y = 2 * x * y + y0;
                x = temp;
                length_squared = x * x + y * y;
                ++repeats;
            }
            image[j * width + i] = repeats;
        }
    }
}
```

# [Pthread] Experiment & Analysis
### 1. Methodology
>#### a. System Spec

QCT Server and Slurm manager provided by the class.

>#### b. Performance Metrics

Use ==Nsight System== provided by NVIDIA for function time measurement.

Command: (use testcase strict34 for example)
```
srun -n1 -c16 \
nsys profile \
    -o "./nsys_reports/hw2a.nsys-rep" \
    --trace nvtx,osrt \ 
    ./hw2a 10000 -0.5506164691618783 -0.5506164628264113 \
    0.6273445437118131 0.6273445403522527 7680 4320
```
---
### 2. Plots: Scalability & Load Balancing & Profile
>#### a. Different Implementation
- **Static continuous**: ```for (int i = start_row; i < end_row; ++i)```
- **Static interleaving**: ```for (int i = thread_id; i < height; i += num_threads)``` 
- **Dynamic**: ```while (true) { if (task_queue->getNextRow() == -1) break; }```

For different threads count, the calculation time: ==dynamic < interleaving < continuous==
![image](https://hackmd.io/_uploads/B1oo2FMWyx.png)

---

>#### b. Different Threads(ncpus) Count

Calculation time basically **doubles down** as thread count doubles up.
![image](https://hackmd.io/_uploads/SkHfCuGb1l.png)

**Speedup** = old calculation time / new calculation time
![image](https://hackmd.io/_uploads/S1xLAdzW1g.png)

---

>#### c. Vectorization degree

Calculation of each row using: 
- __m512d **x1** => 8 doubles
- __m512d **x2** => 16 doubles
- __m512d **x3** => 24 doubles
- __m512d **x4** => 32 doubles

It shows that vectorization can highly improve judging time.

![image](https://hackmd.io/_uploads/rkJrC04WJx.png)


---

### 3. Discussion

- **Scalabitily**: CPU time decreases as the # of threads goes up. The speedup is closed to thread count since there's less commnucation time as in MPI program. The limit of vectorization is about ==4 times== the AVX-512 instructions ==(32 doubles)==, the effect is not significant after this amount. Possible reason being the larger the chunk size, the more imbalance it becomes.
- **Load balance**: In pthread version, dynamic load balancing can be done by using ==pthread_mutex_t==. Before creating worker threads, the master thread first creates a task queue. And since it's created before multithreads, it is stored in ==shared memory== where every thread has access to; thus by using a mutex for ==task requesting== section (critical section), the race condition can be addressed.

---

# [Hybrid] Implementation
### 0. Diagram (3 different implementation)
- ==Static continuous load== (processes) + dynamic load (omp threading)
![S__10715146](https://hackmd.io/_uploads/Bk4z0tGZyx.jpg)

- ==Static interleaving load== (processes) + dynamic load (omp threading)
![S__10715147](https://hackmd.io/_uploads/rknNAFfW1e.jpg)

- ==Dynamic load== (processes) + dynamic load (omp threading)
![S__10715148](https://hackmd.io/_uploads/BkoU0KfZJx.jpg)


### 1. Row distribution for processes
- Use ==Static Interleaving== for distribution.
- Use ==schedule(dynamic, 1)== for dynamic load balancing within threads.
- ```global_j``` is for row distribution, and ```j``` is for local buffer storage.
```
#pragma omp parallel for schedule(dynamic, 1)
for (int global_j = rank; global_j < height; global_j += size)
{
    int j = (global_j - rank) / size;
    double y0 = global_j * y_offset + lower;

    /* Other calculation work */
}
```

### 2. Local buffer size & Gathering
- Use ==(height + size - 1) / size== for ceiling calculation.
- Use ==MPI_Gather== for collecting computed data.
```
// Divide the work
int remainder = height % size;
int num_rows = (height + size - 1) / size; // Get the ceiling of (height / size)

// Calculate Mandelbrot set
MandelbrotGenerator mandelbrot(left, right, lower, upper, width, height, iters, rank, size, num_rows);
std::shared_ptr<int[]> buffer = mandelbrot.generate();

// Prepare for gathering
std::unique_ptr<int[]> image;
if (rank == 0)
    image = std::make_unique<int[]>(width * num_rows * size);

MPI_Gather(buffer.get(), num_rows * width, MPI_INT,
           image.get(), num_rows * width, MPI_INT,
           0, MPI_COMM_WORLD);
```

### 3. Write PNG
- Since it's interleaving computation, the gatherd data will be in ==transposed== state.
![S__10715149](https://hackmd.io/_uploads/HyT-rczZ1g.jpg)
- Thus in PNGWriter, it needs to be transposed back with each row access.
```
int converted_base_y = (base_y % size) * num_rows + base_y / size;
int p = buffer[converted_base_y * width + x];
```

# [Hybrid] Experiment & Analysis
### 1. Methodology
>#### a. System Spec

QCT Server and Slurm manager provided by the class.

>#### b. Performance Metrics

Use ==Nsight System== provided by NVIDIA for function time measurement.

Command: (use testcase strict34 for example)
```
srun -n4 -c16 \
nsys profile \
    -o "./nsys_reports/hw2b_$PMI_RANK.nsys-rep" \
    --mpi-impl openmpi \
    --trace mpi,ucx,nvtx,osrt \
    ./hw2b 10000 -0.5506164691618783 -0.5506164628264113 \
    0.6273445437118131 0.6273445403522527 7680 4320
```
---
### 2. Plots: Scalability & Load Balancing & Profile
>#### a. Different Implementation
- **Static continuous**: ```for (int i = start_row; i < end_row; ++i)```
- **Static interleaving**: ```for (int i = rank; i < height; i += size)``` 
- **Dynamic**: Master process for work distribution.

For different processes count, the calculation time: ==interleaving < continuous < dynamic==.
The main reason dynamic load balancing with MPI is slower is that one process needs to act as a master that distributes the work, resulting in one less process (not to mention it's threads) available for calculation.
The chart below shows that in 4 processes scenario, the calculation time without dynamic MPI load balancing is 1/4 faster than that with dynamic MPI load balancing.
![image](https://hackmd.io/_uploads/BkSvajNWkx.png)
![image](https://hackmd.io/_uploads/B1ZKTsNWkx.png)

---

>#### b. Different Processes Count (Thread count = 4)

In the beginning, calculation time **doubles down** as process count doubles up; however in the later stage, the time reduced is not significant due to communication overhead.

**Worker time** = calculation time + communication time
![image](https://hackmd.io/_uploads/rJVq53NZ1x.png)

**Speedup** = old worker time / new worker time
![image](https://hackmd.io/_uploads/r1sN3nNbye.png)

---

### 3. Discussion
- **Scalabitily**: CPU time decreases as the ==# of process==, ==# of threads==and ==vectorization degree== goes up. Since the communication between processes is not heavy, it barely affects the performance, meaning having more ==total threads== (process_count x thread_count) results in a better performance.
- **Load balance**: There are two layers of load balancing, one is at ==MPI level==, and another is at ==omp level==. In MPI level, we can see that static continuous load has higher probability of getting imbalanced, while static interleaving load addresses the problem pretty well due to the fact that ==neighboring data in Mandelbrot set is relevant==. If one row is calculation intensive, it's neighbor is likely to be calculation intensive.
The thread level load balancing is done by schedule(dynamic), which doesn't need further improvement since we can perform vectorization for calculation in each row.
![S__10747909](https://hackmd.io/_uploads/SJImmkrZkg.jpg)
![S__10747911](https://hackmd.io/_uploads/SJN4S1B-ke.jpg)

# Conclusion
1. In pthread version, the performance is based on ==threads count==, ==balance policy== and ==vectorization degree==. Using dynamic load balancing in this case can highly improve the imbalance of task due to the wide range of calculation time for Mandelbrot set. Futhermore, vectorization degree serves as another way of parallelism for sequential section in each thread.
2. In hybrid (MPI + OpenMP) version, the load balancing is done by ==schedule(dynamic)== with multithreads; therefore whether perform ==static continuous load==, ==static interleaving load== or ==dynamic load== does not have a huge impact on the performance. However, when using dynamic load balancing with MPI processes, one process needs to be ==sacrificed as monitor==, which cannot do the calculation, resulting in performance loss.

